import { neon } from "@neondatabase/serverless"

const sql = neon(process.env.DATABASE_URL!)

// æ•°æ®æ¸…ç†é…ç½®
const CLEANUP_CONFIG = {
  // ä¿ç•™åŸå§‹æ•°æ®çš„æ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
  KEEP_ORIGINAL_HOURS: 1,
  // æ‰¹é‡åˆ é™¤å¤§å°
  BATCH_SIZE: 1000,
  // æœ€å¤§åˆ é™¤æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
  MAX_DELETE_BATCHES: 100
}

export interface CleanupResult {
  success: boolean
  deletedRows: number
  compressedRows: number
  error?: string
  stats: {
    originalRecordsBeforeCleanup: number
    originalRecordsAfterCleanup: number
    compressedRecords: number
    cleanupThreshold: string
  }
}

/**
 * æ‰§è¡Œæ•°æ®æ¸…ç†
 * 1. å‹ç¼©æ—§æ•°æ®
 * 2. åˆ é™¤å·²å‹ç¼©çš„åŸå§‹æ•°æ®
 * 3. åªä¿ç•™æœ€è¿‘1å°æ—¶çš„åŸå§‹æ•°æ®
 */
export async function performDataCleanup(): Promise<CleanupResult> {
  try {
    console.log('ğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...')
    
    // è·å–æ¸…ç†å‰çš„ç»Ÿè®¡ä¿¡æ¯
    const statsBefore = await getDataStats()
    
    // è®¡ç®—æ¸…ç†é˜ˆå€¼ï¼ˆä¿ç•™æœ€è¿‘1å°æ—¶çš„æ•°æ®ï¼‰
    const cleanupThreshold = new Date(Date.now() - CLEANUP_CONFIG.KEEP_ORIGINAL_HOURS * 60 * 60 * 1000)
    const cleanupThresholdISO = cleanupThreshold.toISOString()
    
    console.log(`ğŸ“… æ¸…ç†é˜ˆå€¼: ${cleanupThresholdISO}`)
    
    // 1. é¦–å…ˆå‹ç¼©æ—§æ•°æ®
    const compressedRows = await compressOldData(cleanupThreshold)
    console.log(`ğŸ—œï¸ å‹ç¼©äº† ${compressedRows} è¡Œæ•°æ®`)
    
    // 2. åˆ é™¤å·²å‹ç¼©çš„åŸå§‹æ•°æ®
    const deletedRows = await deleteOldOriginalData(cleanupThreshold)
    console.log(`ğŸ—‘ï¸ åˆ é™¤äº† ${deletedRows} è¡ŒåŸå§‹æ•°æ®`)
    
    // è·å–æ¸…ç†åçš„ç»Ÿè®¡ä¿¡æ¯
    const statsAfter = await getDataStats()
    
    const result: CleanupResult = {
      success: true,
      deletedRows,
      compressedRows,
      stats: {
        originalRecordsBeforeCleanup: statsBefore.originalRecords,
        originalRecordsAfterCleanup: statsAfter.originalRecords,
        compressedRecords: statsAfter.compressedRecords,
        cleanupThreshold: cleanupThresholdISO
      }
    }
    
    console.log('âœ… æ•°æ®æ¸…ç†å®Œæˆ:', result)
    return result
    
  } catch (error) {
    console.error('âŒ æ•°æ®æ¸…ç†å¤±è´¥:', error)
    return {
      success: false,
      deletedRows: 0,
      compressedRows: 0,
      error: String(error),
      stats: {
        originalRecordsBeforeCleanup: 0,
        originalRecordsAfterCleanup: 0,
        compressedRecords: 0,
        cleanupThreshold: ''
      }
    }
  }
}

/**
 * å‹ç¼©æ—§æ•°æ®
 */
async function compressOldData(beforeTime: Date): Promise<number> {
  // ç¡®ä¿å‹ç¼©è¡¨å­˜åœ¨
  await ensureCompressionTableExists()
  
  // æŒ‰5åˆ†é’Ÿé—´éš”å‹ç¼©æ•°æ®
  const compressResult = await sql`
    INSERT INTO onenet_data_compressed (
      device_id, datastream_id, avg_value, min_value, max_value, 
      sample_count, time_bucket
    )
    SELECT 
      device_id,
      datastream_id,
      AVG(CAST(value AS NUMERIC)) as avg_value,
      MIN(CAST(value AS NUMERIC)) as min_value,
      MAX(CAST(value AS NUMERIC)) as max_value,
      COUNT(*) as sample_count,
      -- 5åˆ†é’Ÿæ—¶é—´åˆ†æ¡¶
      date_trunc('hour', created_at) +
      INTERVAL '5 minutes' * FLOOR(EXTRACT(minute FROM created_at) / 5) as time_bucket
    FROM onenet_data 
    WHERE created_at < ${beforeTime.toISOString()}
    GROUP BY device_id, datastream_id, time_bucket
    ON CONFLICT (device_id, datastream_id, time_bucket) 
    DO UPDATE SET
      avg_value = EXCLUDED.avg_value,
      min_value = EXCLUDED.min_value,
      max_value = EXCLUDED.max_value,
      sample_count = EXCLUDED.sample_count
  `
  
  return compressResult.length || 0
}

/**
 * åˆ é™¤å·²å‹ç¼©çš„åŸå§‹æ•°æ®
 */
async function deleteOldOriginalData(beforeTime: Date): Promise<number> {
  let totalDeleted = 0
  let batchCount = 0
  
  while (batchCount < CLEANUP_CONFIG.MAX_DELETE_BATCHES) {
    // æ‰¹é‡åˆ é™¤ä»¥é¿å…é•¿æ—¶é—´é”å®š
    const deleteResult = await sql`
      DELETE FROM onenet_data 
      WHERE id IN (
        SELECT id FROM onenet_data 
        WHERE created_at < ${beforeTime.toISOString()}
        LIMIT ${CLEANUP_CONFIG.BATCH_SIZE}
      )
    `
    
    const deletedInBatch = deleteResult.length || 0
    totalDeleted += deletedInBatch
    batchCount++
    
    console.log(`ğŸ—‘ï¸ æ‰¹æ¬¡ ${batchCount}: åˆ é™¤äº† ${deletedInBatch} è¡Œ`)
    
    // å¦‚æœè¿™ä¸€æ‰¹åˆ é™¤çš„è¡Œæ•°å°‘äºæ‰¹é‡å¤§å°ï¼Œè¯´æ˜å·²ç»åˆ é™¤å®Œäº†
    if (deletedInBatch < CLEANUP_CONFIG.BATCH_SIZE) {
      break
    }
    
    // çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦å ç”¨æ•°æ®åº“èµ„æº
    await new Promise(resolve => setTimeout(resolve, 100))
  }
  
  return totalDeleted
}

/**
 * ç¡®ä¿å‹ç¼©è¡¨å­˜åœ¨
 */
async function ensureCompressionTableExists(): Promise<void> {
  await sql`
    CREATE TABLE IF NOT EXISTS onenet_data_compressed (
      id SERIAL PRIMARY KEY,
      device_id VARCHAR(50),
      datastream_id VARCHAR(100),
      avg_value NUMERIC,
      min_value NUMERIC,
      max_value NUMERIC,
      sample_count INTEGER,
      time_bucket TIMESTAMP,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      UNIQUE(device_id, datastream_id, time_bucket)
    )
  `
  
  // åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
  await sql`
    CREATE INDEX IF NOT EXISTS idx_compressed_device_datastream_time 
    ON onenet_data_compressed (device_id, datastream_id, time_bucket)
  `
}

/**
 * è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
 */
async function getDataStats() {
  const originalCount = await sql`SELECT COUNT(*) as count FROM onenet_data`
  const compressedCount = await sql`SELECT COUNT(*) as count FROM onenet_data_compressed`
  
  return {
    originalRecords: parseInt(originalCount[0].count),
    compressedRecords: parseInt(compressedCount[0].count)
  }
}

/**
 * è·å–æ•°æ®åˆ†å¸ƒç»Ÿè®¡
 */
export async function getDataDistributionStats() {
  try {
    const now = new Date()
    const oneHourAgo = new Date(now.getTime() - 60 * 60 * 1000)
    const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000)
    const oneWeekAgo = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000)
    
    // åŸå§‹æ•°æ®åˆ†å¸ƒ
    const originalStats = await sql`
      SELECT 
        COUNT(CASE WHEN created_at >= ${oneHourAgo.toISOString()} THEN 1 END) as last_hour,
        COUNT(CASE WHEN created_at >= ${oneDayAgo.toISOString()} THEN 1 END) as last_day,
        COUNT(CASE WHEN created_at >= ${oneWeekAgo.toISOString()} THEN 1 END) as last_week,
        COUNT(*) as total
      FROM onenet_data
    `
    
    // å‹ç¼©æ•°æ®åˆ†å¸ƒ
    const compressedStats = await sql`
      SELECT 
        COUNT(CASE WHEN time_bucket >= ${oneHourAgo.toISOString()} THEN 1 END) as last_hour,
        COUNT(CASE WHEN time_bucket >= ${oneDayAgo.toISOString()} THEN 1 END) as last_day,
        COUNT(CASE WHEN time_bucket >= ${oneWeekAgo.toISOString()} THEN 1 END) as last_week,
        COUNT(*) as total
      FROM onenet_data_compressed
    `
    
    return {
      original: {
        lastHour: parseInt(originalStats[0].last_hour),
        lastDay: parseInt(originalStats[0].last_day),
        lastWeek: parseInt(originalStats[0].last_week),
        total: parseInt(originalStats[0].total)
      },
      compressed: {
        lastHour: parseInt(compressedStats[0].last_hour),
        lastDay: parseInt(compressedStats[0].last_day),
        lastWeek: parseInt(compressedStats[0].last_week),
        total: parseInt(compressedStats[0].total)
      },
      cleanupThreshold: oneHourAgo.toISOString()
    }
  } catch (error) {
    console.error('è·å–æ•°æ®åˆ†å¸ƒç»Ÿè®¡å¤±è´¥:', error)
    return null
  }
}

/**
 * æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
 */
export async function shouldPerformCleanup(): Promise<boolean> {
  try {
    const stats = await getDataDistributionStats()
    if (!stats) return false
    
    // å¦‚æœåŸå§‹æ•°æ®è¡¨ä¸­æœ‰è¶…è¿‡1å°æ—¶çš„æ•°æ®ï¼Œå°±éœ€è¦æ¸…ç†
    const oldDataCount = stats.original.total - stats.original.lastHour
    return oldDataCount > 0
  } catch (error) {
    console.error('æ£€æŸ¥æ¸…ç†éœ€æ±‚å¤±è´¥:', error)
    return false
  }
}
